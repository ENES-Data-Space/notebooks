{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4632f6-178b-4c9e-8934-ce3419f747a5",
   "metadata": {},
   "source": [
    "### Getting started with PyOphidia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0146a1-73f9-468e-90a9-94aa3a7f887c",
   "metadata": {},
   "source": [
    "This notebook provides some basic examples of how to use the ophidia framework features for climate data analysis and, in particular, it shows some of the main commands from the PyOphidia module. \n",
    "\n",
    "**PyOphidia** is a GPLv3-licensed Python module to interact with the Ophidia framework. It implements two main classes:\n",
    "   \n",
    "- **Client** class: it supports the submissions of Ophidia commands and workflows as well as the management of sessions from Python code (similar to the Ophidia Terminal)\n",
    "   - It allows running all the Ophidia operators, including massive tasks and workflows\n",
    "   \n",
    "- **Cube** class: it builds on the client class and provides the datacube type abstraction and the methods to manipulate, process and get information on cubes objects and \n",
    "   - It defines a object-oriented approach to handle the datacubes more naturally\n",
    "   \n",
    "While the *cube* module provides a user-friendly interface, the *client* module allows a finer specification of the operators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fcf563-75cd-4269-923d-72c8f5b15ec2",
   "metadata": {},
   "source": [
    "First of all import PyOphidia modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695e070-e098-43ea-8601-b2da852212a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyOphidia import cube, client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff5ec7f-5ed0-4373-a374-c041ed8d4c78",
   "metadata": {},
   "source": [
    "Then, setup a connection to the server instance. Connection details can be automatically inferred from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575ffc0-a3fe-4434-b9cf-801927536bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.setclient(read_env=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fd69e-1bc4-4783-a062-fedc835dcef5",
   "metadata": {},
   "source": [
    "Let's now load a NetCDF file. We can inspect the file with the *explorenc* Ophidia operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f639b-01f3-497e-bcd3-4ec9104a46b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "input_file=home+\"/data/CMIP6/ScenarioMIP/CMCC/CMCC-CM2-SR5/ssp585/r1i1p1f1/Amon/tas/gn/v20200622/tas_Amon_CMCC-CM2-SR5_ssp585_r1i1p1f1_gn_201501-210012.nc\"\n",
    "\n",
    "cube.Cube.explorenc(src_path=input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878f87b-834d-4f93-99a0-3f26d315d590",
   "metadata": {},
   "source": [
    "We can now create a datacube from the NetCDF file:\n",
    "- The file is located in the home folder under **data/CMIP6/ScenarioMIP/CMCC/CMCC-CM2-SR5/ssp585/r1i1p1f1/Amon/tas/gn/v20200622/tas_Amon_CMCC-CM2-SR5_ssp585_r1i1p1f1_gn_201501-210012.nc**\n",
    "- The variable to be imported is **tas**\n",
    "- Data should be arranged in order to operate on time series (**time** dimension) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bceb703-a9a7-46c2-ac22-3965ee6cf30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mycube = cube.Cube.importnc2(\n",
    "                src_path=input_file,\n",
    "                measure='tas',\n",
    "                imp_dim='time',\n",
    "                ncores=1,\n",
    "                nfrag=1,\n",
    "                description=\"Imported cube\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7254713-03aa-4169-b856-66ac0052c31d",
   "metadata": {},
   "source": [
    "Inspect the cube and its dimensions structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b664ecd-2454-44b7-ab89-f80ad1aaf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycube.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa5fb2-69af-4c72-ab2e-68feea86c52e",
   "metadata": {},
   "source": [
    "Check the datacubes available in the virtual file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47739344-017d-44ec-af16-dca56c58db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eda85a3-e752-4d16-93a2-5b224ac797b8",
   "metadata": {},
   "source": [
    "Subset the datacube over time\n",
    "\n",
    "**Note: each instance method produces a new datacube object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee6d8a-a581-496e-af1a-d225ce2e889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycube2 = mycube.subset(\n",
    "                subset_dims=\"time\",\n",
    "                subset_filter=\"1:60\",\n",
    "                subset_type=\"index\",\n",
    "                time_filter=\"yes\",\n",
    "                ncores=1,\n",
    "                description=\"Subsetted cube\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14dca8-2824-44be-aa45-8b79494fd73e",
   "metadata": {},
   "source": [
    "We can use the explore method to check the content of the datacube. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969282a-ecc2-46db-8c38-8b4405d84319",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycube2.explore(limit_filter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fee9dd-9b10-456a-b843-7a21aeafbb67",
   "metadata": {},
   "source": [
    "What if we want to explore just a specific portion of the datacube?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d0fbd-e86f-417f-b53a-60b93ba18141",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycube2.explore(subset_dims=\"lat|lon|time\",subset_type=\"index\",subset_filter=\"1:2|1:4|1:4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b73a62-3d64-4e2e-9552-fba164255fd7",
   "metadata": {},
   "source": [
    "Compute the **maximum** value over the time series for each point in the spatial domain. We can also compute other metrics (see http://ophidia.cmcc.it/documentation/users/operators/OPH_REDUCE.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d68925-5d51-4336-bc19-ed4500a38a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycube3 = mycube2.reduce(\n",
    "    operation='max',\n",
    "    ncores=1,\n",
    "    description=\"Reduced cube\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d67ecd-3c0c-4062-8b9b-b2b997f332a3",
   "metadata": {},
   "source": [
    "Let's export the data into a Python-friendly structure. \n",
    "\n",
    "**Note: this is the first time we move data from the server-side to the Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678d6c6c-55c3-4baa-829a-b24a580ffb85",
   "metadata": {},
   "source": [
    "The structure looks something like this\n",
    "\n",
    "<img src=\"imgs/export_array.png\" alt=\"Export Array\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f093195-5a09-4c71-a1cf-83ca8e7c66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mycube3.export_array()\n",
    "\n",
    "from IPython.lib.pretty import pprint\n",
    "pprint(data['dimension'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc438d87-70c4-42a7-9185-e205191adb13",
   "metadata": {},
   "source": [
    "The data exported in the Python structure can be used to create a map (note the definition of a Python function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f881fc4-7925-49e7-8832-06ebf67f609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plotData(data):\n",
    "    \n",
    "    import cartopy.crs as ccrs\n",
    "    import matplotlib.pyplot as plt\n",
    "    from cartopy.mpl.geoaxes import GeoAxes\n",
    "    from cartopy.util import add_cyclic_point\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6), dpi=100)\n",
    "\n",
    "    #Add Geo axes to the figure with the specified projection (PlateCarree)\n",
    "    projection = ccrs.PlateCarree()\n",
    "    ax = plt.axes(projection=projection)\n",
    "\n",
    "    #Draw coastline and gridlines\n",
    "    ax.coastlines()\n",
    "\n",
    "    gl = ax.gridlines(crs=projection, draw_labels=True, linewidth=1, color='black', alpha=0.9, linestyle=':')\n",
    "    gl.xlabels_top = False\n",
    "    gl.ylabels_right = False\n",
    "\n",
    "    lat = data['dimension'][0]['values'][ : ]\n",
    "    lon = data['dimension'][1]['values'][ : ]\n",
    "    var = data['measure'][0]['values'][ : ]\n",
    "    var = np.reshape(var, (len(lat), len(lon)))\n",
    "\n",
    "    #Wraparound points in longitude\n",
    "    var_cyclic, lon_cyclic = add_cyclic_point(var, coord=np.asarray(lon))\n",
    "    x, y = np.meshgrid(lon_cyclic,lat)\n",
    "\n",
    "    #Define color levels for color bar\n",
    "    clevs = np.arange(200,340,2)\n",
    "\n",
    "    #Set filled contour plot\n",
    "    cnplot = ax.contourf(x, y, var_cyclic, clevs, transform=projection,cmap=plt.cm.jet)\n",
    "    plt.colorbar(cnplot,ax=ax)\n",
    "\n",
    "    ax.set_aspect('auto', adjustable=None)\n",
    "\n",
    "    plt.title('Maximum Near-Surface Air Temperature (deg K)')\n",
    "    plt.show()\n",
    "    \n",
    "plotData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e04c5-9589-4f53-85ec-7464f9b82b73",
   "metadata": {},
   "source": [
    "Our workspace now contains several datacubes from the experiments just run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90362c-7a6f-4eb3-ae1e-5d7ecc59b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df8f08-dcff-46c8-bc95-615568bd8acd",
   "metadata": {},
   "source": [
    "Once done, we can clear the space before moving to other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84551611-309a-40fb-b2a7-72218feeff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.deletecontainer(container='tas_Amon_CMCC-CM2-SR5_ssp585_r1i1p1f1_gn_201501-210012.nc',force='yes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a73579-f356-433a-862f-1f8e8aa28fa8",
   "metadata": {},
   "source": [
    "The virtual file system should now be \"clean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6235002-1965-4038-b06e-0de88f010ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.Cube.list(level=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
